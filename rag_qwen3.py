#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
import glob
import chardet
import shutil
from typing import List, Optional
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import (
    PyMuPDFLoader,
    TextLoader,
    UnstructuredWordDocumentLoader
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.exceptions import OutputParserException
from langchain_core.runnables import RunnableSequence
from langchain.schema import Document

# ----------------------------
# AYARLAR
# ----------------------------
DOCUMENTS_DIR = "C:/FIZIK_RAG/RAG_documents"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 300
TOP_K = 10
VECTOR_DB_PATH = "vector_db"

# Model ve Embedding AyarlarÄ±
llm = OllamaLLM(
    model="qwen3:8b",
    temperature=0.7,
    num_predict=3000,
    system="KÄ±sa ve Ã¶z cevaplar ver. Sadece belgelerdeki bilgiye gÃ¶re yanÄ±tla."
)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": False}
)

# ----------------------------
# TÃœRKÃ‡E KARAKTER DÃœZELTME
# ----------------------------
def fix_turkish_text(text: str) -> str:
    replacements = {
        'â”€â–’': 'Ä±', 'â”œâ•': 'Ã¼', 'â”œÃ¶': 'Ã¶', 'â”œÄŸ': 'ÄŸ',
        'â”œÅŸ': 'ÅŸ', 'â”œÃ§': 'Ã§', 'â”€Ã—': 'Ä', 'â”€â–‘': 'Ä°',
        'â”œÄ±': 'Ä±', 'â”œÂº': 'ÅŸ', 'â”œÂ®': 'Ã©', 'â”œÃ±': 'Ã¤',
        'â”œÃ¡': 'Ã¡', 'â”œÂ¡': 'Ã­',
    }
    for broken, correct in replacements.items():
        text = text.replace(broken, correct)
    return text

def detect_encoding(file_path: str) -> str:
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)
    result = chardet.detect(raw_data)
    return result['encoding'] or 'utf-8'

# ----------------------------
# BELGE YÃœKLEME
# ----------------------------
def load_text_file_with_encoding(filepath: str) -> List[Document]:
    try:
        encoding = detect_encoding(filepath)
        loader = TextLoader(filepath, encoding=encoding)
        docs = loader.load()
        for doc in docs:
            doc.page_content = fix_turkish_text(doc.page_content)
        return docs
    except Exception as e:
        logging.error(f"TXT yÃ¼kleme hatasÄ± ({filepath}): {e}")
        return []

def load_pdf_with_fix(filepath: str) -> List[Document]:
    try:
        loader = PyMuPDFLoader(filepath)
        docs = loader.load()
        for doc in docs:
            doc.page_content = fix_turkish_text(doc.page_content)
        return docs
    except Exception as e:
        logging.error(f"PDF yÃ¼kleme hatasÄ± ({filepath}): {e}")
        return []

def load_local_documents() -> List[Document]:
    documents = []

    # 1. Ã–NCELÄ°KLÄ°: oku.txt dosyasÄ±nÄ± ayrÄ± yÃ¼kle
    oku_txt_path = os.path.join(DOCUMENTS_DIR, "oku.txt")
    if os.path.exists(oku_txt_path):
        oku_docs = load_text_file_with_encoding(oku_txt_path)
        if oku_docs:
            # Her bir satÄ±rÄ± ayrÄ± bir belge olarak iÅŸle
            lines = oku_docs[0].page_content.split('\n')
            for line in lines:
                if line.strip():  # BoÅŸ satÄ±rlarÄ± atla
                    documents.append(Document(page_content=line.strip()))
            logging.info(f"Ã–ZEL: oku.txt'den {len(documents)} tanÄ±m yÃ¼klendi")
            print(f"ğŸ“„ oku.txt iÃ§eriÄŸi:")
            for i, doc in enumerate(documents):
                print(f"  {i+1}. {doc.page_content}")

    # 2. DiÄŸer belgeleri normal ÅŸekilde yÃ¼kle
    pdf_files = glob.glob(os.path.join(DOCUMENTS_DIR, "**/*.pdf"), recursive=True)
    txt_files = glob.glob(os.path.join(DOCUMENTS_DIR, "**/*.txt"), recursive=True)
    word_files = glob.glob(os.path.join(DOCUMENTS_DIR, "**/*.doc*"), recursive=True)

    # PDF'leri yÃ¼kle
    for pdf_file in pdf_files:
        docs = load_pdf_with_fix(pdf_file)
        documents.extend(docs)
        logging.info(f"YÃ¼klendi: {pdf_file} ({len(docs)} sayfa)")

    # TXT'leri yÃ¼kle (oku.txt hariÃ§)
    for txt_file in txt_files:
        if txt_file == oku_txt_path:
            continue
        docs = load_text_file_with_encoding(txt_file)
        documents.extend(docs)
        logging.info(f"YÃ¼klendi: {txt_file} ({len(docs)} belge)")

    # Word belgelerini yÃ¼kle
    for word_file in word_files:
        try:
            loader = UnstructuredWordDocumentLoader(word_file)
            docs = loader.load()
            for doc in docs:
                doc.page_content = fix_turkish_text(doc.page_content)
            documents.extend(docs)
            logging.info(f"YÃ¼klendi: {word_file} ({len(docs)} belge)")
        except Exception as e:
            logging.error(f"Word yÃ¼kleme hatasÄ± ({word_file}): {e}")

    return documents

# ----------------------------
# VEKTÃ–R VERÄ°TABANI
# ----------------------------
def create_local_vector_db() -> Optional[FAISS]:
    documents = load_local_documents()
    if not documents:
        logging.error("HiÃ§bir belge yÃ¼klenemedi!")
        return None

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    splits = splitter.split_documents(documents)
    logging.info(f"Toplam {len(documents)} belgeden {len(splits)} parÃ§a oluÅŸturuldu")

    try:
        vector_db = FAISS.from_documents(splits, embeddings)
        logging.info("VektÃ¶r veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu")
        return vector_db
    except Exception as e:
        logging.error(f"VektÃ¶r veritabanÄ± oluÅŸturma hatasÄ±: {e}")
        return None

# ----------------------------
# DISK Ä°ÅLEMLERÄ°
# ----------------------------
def save_vector_db(db: FAISS, path: str):
    """FAISS veritabanÄ±nÄ± diske kaydeder"""
    if not os.path.exists(path):
        os.makedirs(path)
    db.save_local(path)
    print(f"âœ… FAISS veritabanÄ± '{path}' konumuna kaydedildi.")

def load_vector_db(path: str) -> FAISS:
    """FAISS veritabanÄ±nÄ± diskten yÃ¼kler"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"FAISS veritabanÄ± '{path}' bulunamadÄ±.")
    print(f"ğŸ“¦ FAISS veritabanÄ± '{path}' konumundan yÃ¼kleniyor...")
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

def create_or_load_vector_db(force_recreate=False) -> Optional[FAISS]:
    """VeritabanÄ±nÄ± diskten yÃ¼kler veya oluÅŸturur"""
    if force_recreate and os.path.exists(VECTOR_DB_PATH):
        shutil.rmtree(VECTOR_DB_PATH)
        print(f"ğŸ—‘ï¸ Mevcut veritabanÄ± silindi, yeniden oluÅŸturulacak.")
    
    if os.path.exists(VECTOR_DB_PATH):
        try:
            return load_vector_db(VECTOR_DB_PATH)
        except Exception as e:
            logging.error(f"VeritabanÄ± yÃ¼kleme hatasÄ±: {e}")
    
    # VeritabanÄ± yoksa veya yÃ¼klenemezse oluÅŸtur
    db = create_local_vector_db()
    if db:
        save_vector_db(db, VECTOR_DB_PATH)
    return db

# ----------------------------
# Ã–NCELÄ°KLENDÄ°RME
# ----------------------------
def prioritize_results(query: str, docs: List[Document]) -> List[Document]:
    """
    Belgeleri Ã¶nceliklendirir:
    1. Tam baÅŸlÄ±k eÅŸleÅŸmeleri (Ã¶r: "hÄ±z:")
    2. KÄ±smi baÅŸlÄ±k eÅŸleÅŸmeleri (Ã¶r: "hÄ±z")
    3. DiÄŸer belgeler
    """
    if not docs:
        return []
    
    # Sorgudan anahtar kelimeleri Ã§Ä±kar
    query_lower = query.lower().rstrip("?").replace(" nedir", "").strip()
    
    # Tam baÅŸlÄ±k eÅŸleÅŸmesi iÃ§in format (Ã¶r: "hÄ±z:")
    exact_keyword = f"{query_lower}:"
    
    # Ã–ncelik sÄ±rasÄ±na gÃ¶re belgeleri ayÄ±r
    exact_matches = []
    partial_matches = []
    others = []
    
    print(f"\nğŸ” Ã–nceliklendirme iÃ§in aranan anahtar kelimeler:")
    print(f"  Tam eÅŸleÅŸme: '{exact_keyword}'")
    print(f"  KÄ±smi eÅŸleÅŸme: '{query_lower}'")
    
    for doc in docs:
        content_lower = doc.page_content.lower()
        
        if exact_keyword in content_lower:
            print(f"âœ… Tam eÅŸleÅŸme bulundu: '{doc.page_content[:50]}...'")
            exact_matches.append(doc)
        elif query_lower in content_lower:
            print(f"âš ï¸ KÄ±smi eÅŸleÅŸme bulundu: '{doc.page_content[:50]}...'")
            partial_matches.append(doc)
        else:
            others.append(doc)
    
    print(f"\nğŸ“Š Ã–nceliklendirme sonuÃ§larÄ±:")
    print(f"  Tam eÅŸleÅŸme: {len(exact_matches)} belge")
    print(f"  KÄ±smi eÅŸleÅŸme: {len(partial_matches)} belge")
    print(f"  DiÄŸer: {len(others)} belge")
    
    # Ã–ncelik sÄ±rasÄ±na gÃ¶re birleÅŸtir
    return exact_matches + partial_matches + others

# ----------------------------
# ARAMA
# ----------------------------
def search_local_knowledge(query: str, k: int = TOP_K) -> str:
    if not local_vector_db:
        return ""
    try:
        fixed_query = fix_turkish_text(query)
        docs = local_vector_db.similarity_search(fixed_query, k=k)
        
        print(f"\nğŸ“ Bulunan {len(docs)} belge (Ã¶nceliklendirme Ã¶ncesi):")
        for i, doc in enumerate(docs):
            print(f"  {i+1}. {doc.page_content[:50]}...")
        
        prioritized = prioritize_results(fixed_query, docs)
        
        print(f"\nâ­ Ã–nceliklendirilmiÅŸ {len(prioritized)} belge:")
        for i, doc in enumerate(prioritized):
            print(f"  {i+1}. {doc.page_content[:50]}...")
        
        return "\n---\n".join(doc.page_content for doc in prioritized)
    except Exception as e:
        logging.error(f"Arama hatasÄ±: {e}")
        return ""

# ----------------------------
# CEVAP ÃœRETME
# ----------------------------
def generate_answer(query: str, local_context: str) -> str:
    if not local_context.strip():
        return "Bilmiyorum"

    prompt = PromptTemplate.from_template(
        """AÅŸaÄŸÄ±daki belgelere dayanarak TÃ¼rkÃ§e olarak soruyu yanÄ±tla:

BELGELER:
{local_context}

SORU: {query}

Kurallar:
- Sadece belgelerdeki bilgilere gÃ¶re yanÄ±tla
- Cevap belgelerde yoksa kesinlikle "Bilmiyorum" de
- KÄ±sa ve Ã¶z cevap ver
- Ek bilgi veya aÃ§Ä±klama ekleme"""
    )

    chain = RunnableSequence(
        prompt | llm | StrOutputParser()
    )

    try:
        return chain.invoke({"query": query, "local_context": local_context})
    except OutputParserException as e:
        logging.error(f"Ã‡Ä±ktÄ± iÅŸleme hatasÄ±: {e}")
        return "Cevap oluÅŸturulurken hata oluÅŸtu"

# ----------------------------
# ANA RAG PIPELINE
# ----------------------------
def local_rag_pipeline(query: str) -> str:
    try:
        local_context = search_local_knowledge(query)
        return generate_answer(query, local_context)
    except Exception as e:
        logging.error(f"RAG iÅŸlem hatasÄ±: {e}")
        return f"Hata: {str(e)}"

# ----------------------------
# TESTLER
# ----------------------------
def test_embedding():
    test_text = "hÄ±z nedir"
    embedding = embeddings.embed_query(test_text)
    print(f"Embedding boyutu: {len(embedding)}")
    print(f"Ã–rnek deÄŸerler: {embedding[:5]}")
    return True

def test_turkish_text():
    broken_text = "hâ”€â–’z: Bir cismin birim zamandaki yer deâ”€ÅŸiâ”¼ÃŸtirmesiyle tanâ”€â–’mlanâ”€â–’r."
    fixed_text = fix_turkish_text(broken_text)
    print(f"Bozuk: {broken_text}")
    print(f"DÃ¼zeltilmiÅŸ: {fixed_text}")
    return "hÄ±z" in fixed_text

# ----------------------------
# ANA PROGRAM
# ----------------------------
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("\n[VektÃ¶r TabanlÄ± RAG Sistemi (TÃ¼rkÃ§e Destekli)]")
    print("Model: qwen3:8b")
    print("Embeddings: sentence-transformers/all-MiniLM-L6-v2\n")

    print("=== SÄ°STEM TESTLERÄ° ===")
    print("1. Embedding testi:", "BaÅŸarÄ±lÄ±" if test_embedding() else "BaÅŸarÄ±sÄ±z")
    print("2. TÃ¼rkÃ§e karakter testi:", "BaÅŸarÄ±lÄ±" if test_turkish_text() else "BaÅŸarÄ±sÄ±z")
    print("=======================\n")

    # VeritabanÄ±nÄ± diskten yÃ¼kle veya oluÅŸtur ve kaydet
    # force_recreate=True ile mevcut veritabanÄ±nÄ± silip yeniden oluÅŸtur
    local_vector_db = create_or_load_vector_db(force_recreate=True)

    while True:
        try:
            query = input("Soru ('Ã§Ä±k' yazarsan Ã§Ä±kÄ±lÄ±r): ").strip()
            if query.lower() in ("Ã§Ä±k", "exit", "quit"):
                break

            print("\nğŸ”„ Cevap oluÅŸturuluyor...")
            response = local_rag_pipeline(query)
            print(f"\nğŸ” Cevap:\n{response}\n")

        except KeyboardInterrupt:
            print("\nKullanÄ±cÄ± tarafÄ±ndan durduruldu.")
            break
        except Exception as e:
            print(f"Beklenmeyen hata: {e}")
            break
